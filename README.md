情感支持对话 (ESC) 是一项旨在通过日常对话缓解个人情绪困扰的任务。鉴于其固有的复杂性和非直观性，ESConv 数据集结合了支持策略以促进生成适当的响应。
最近，尽管大型语言模型 (LLM) 具有出色的对话能力，但先前的研究表明，它们往往难以提供有用的情感支持。因此，这项工作首先分析了 LLM 在 ESConv 上的结果，揭示了选择正确策略的挑战以及对特定策略的明显偏好。
受此启发，我们探索了 LLM 中固有偏好对提供情感支持的影响，因此，我们观察到表现出对特定策略的高度偏好会阻碍有效的情感支持，从而加剧其在预测适当策略方面的稳健性。此外，我们进行了一项方法论研究，以深入了解法学硕士成为熟练情感支持者的必要方法。我们的研究结果强调：（1）对特定策略的低偏好阻碍了情感支持的进展，（2）外部援助有助于减少偏好偏差，（3）现有的法学硕士无法独自成为良好的情感支持者。这些见解为未来研究提供了有希望的途径，以提高法学硕士的情商。

总而言之，我们的贡献如下：• 我们介绍了各种 LLM 对策略的不同偏好。• 我们提出了一套新的指标，重点关注策略：熟练程度、偏好和偏好偏差。• 我们强调偏好偏差在各个阶段提供有效情感支持方面的关键作用。• 我们展示了 LLM 与接触假设相一致，这表明外部援助可以帮助解决偏好偏差。• 我们构建了一套全面的标准来准确评估回应是否提供了有用的情感支持。• 通过广泛的人工评估，我们证明减轻偏好偏差对于减少低质量回应的比例至关重要，从而提供有效的情感支持。



前期工作与相关工作
2.1 情感支持对话
Liu 等人 (2021) 提出了情感支持对话任务并发布了数据集 ESConv，涵盖了广泛的情况。ESC 以经历情绪困扰的用户 (求助者) 与旨在提供安慰的系统 (支持者) 之间的互动为中心，旨在缓解用户的情绪强度。由于 ESC 主要侧重于提供情感支持，因此它不同于专业咨询，而是强调在社交背景下的支持，例如与朋友或家人的互动。
ESConv 中的情感支持程序通常遵循三个阶段 (探索 → 安慰 → 行动)。虽然它不一定遵循这个阶段的顺序，但提供情感支持通常需要经历多个阶段。因此，能够在所有阶段提供适当的回应至关重要，因为特定阶段的表现不佳可能会阻碍对话的进展。有关 ESConv 的更多详细信息，请参阅附录 B。


2.2 将策略纳入 ESC 系统

先前关于构建 ESC 系统的研究主要强调支持策略的整合，结合情感、语义（Zhao 等，2023b）和角色（Cheng 等，2023）等元素。一些潜在的研究侧重于对用户状态和策略进行建模（Cheng 等，2022；Jia 等，2023）。值得注意的是，Deng 等（2023）将生成常识知识模型（Hwang 等，2020）与策略预测结合起来作为辅助任务，以提供更好的情感支持。

然而，这些方法中的许多都涉及对模型架构的修改或调整预训练参数，而这个过程通常对于 LLM 来说是不可行的。
2.3 LLM 的情感支持
随着 LLM 的出现，越来越多的研究探索 LLM 作为情感支持者。最近的研究试图通过情境学习来提示 LLM，以利用 LLM 作为 ESC 系统 (Chen 等人，2023a;
Zheng 等人，2023b)，从而取代微调方法。尽管 LLM 具有潜力，但最近的研究表明 LLM 在提供情感支持方面的能力存在局限性 (Chung 等人，2023; Farhat，2023; Eshghie 和 Eshghie，2023;
Song 等人，2024)。具体而言，Song 等人 (2024) 发现，由于 LLM 对情感支持反应的建议缺乏责任感，用户可能会感到不适或担忧。然而，尽管大多数 ESC 研究都集中于在其方法中利用支持策略，但针对 LLM 中的策略的全面分析尚未得到充分探索。

3 评估设置
3.1 任务和重点
任务：情感支持响应生成。
机器生成的响应在提供情感支持方面的有效性在很大程度上取决于选择适当的策略。我们将情感支持响应生成任务制定为通过支持策略生成响应。
正式地，给定对话背景 I、来自寻求者的聊天前调查（例如，情感、情况）和对话上下文 C，模型 θ 首先预测策略 S，然后根据 I、C 和 S 生成响应 R：
S ∼ Pθ(·|I, C) (1)
R ∼ Pθ(·|I, C, S) (2)
重点：以策略为中心的分析。在 LLM 难以提供情感支持的各种原因中，这项工作重点关注策略，这是 ESC 系统中的关键因素。为了强调以策略为中心的分析的有效性，我们探索了基于真实策略生成的响应质量的潜力。结果，在图 2 中，如果模型能够正确预测策略，那么情感支持响应的质量就有很大提升空间。

3.2 评估集
为了进行全面分析，我们从 ESConv 中构建了三个基于阶段的测试集 Dt，如表 1 所示。首先，我们将对话随机截断为 5-15 轮样本。然后，我们为每个样本标注一个阶段，并根据其阶段对样本进行分类。此外，我们尽量减少策略 Others 的比例，以减少与情感支持不太相关的回应。
最后，我们删除一些样本，以确保每个测试集中的对话没有重叠，有关数据构造的更详细说明，请参阅附录 B.2。
3.3 指标
熟练程度。我们将熟练程度定义为模型选择正确策略的能力。策略 (qi) 的熟练程度量化为策略 i 的 F1 分数。为了准确分析模型的熟练程度，我们使用两种类型的 F1 分数，它们都源自每种策略的熟练程度 qi：(1) 宏观 F1 分数 Q，和 (2) 加权 F1 分数。宏观 F1 分数 (Q) 表示模型在所有策略中的整体熟练程度，该分数在整个测试集 (D) 上进行评估。相反，我们使用加权 F1 分数来评估仅由特定阶段对应的数据组成的测试集 (Dt) 上的模型。偏好。我们将偏好定义为模型对某些策略的偏好程度。为了量化 LLM 中每种策略的偏好，我们采用了 Bradley-Terry 模型 (Bradley and Terry, 1952)，该模型在人类偏好建模中得到广泛使用 (Rafailov et al., 2023)。按照 Newman (2023) 的说法，我们正式推导出策略 i 的偏好 p，如下所示：

4 LLM 的熟练程度和策略偏好
4.1 模型和实施细节
根据其可用性，我们将 LLM 分为以下两类：（1）可通过 API 获得的闭源模型，例如 Chat-
GPT 和 GPT4 (OpenAI, 2023b)；（2）可通过参数访问的开源模型，包括
LLaMA2-7B/70B (Touvron et al., 2023)、Tulu-
70B (Ivison et al., 2023)、Vicuna-13B (Zheng et al.,
2023a)、Solar-10.7B (Kim et al., 2023) 和 Mistral-
7B (Jiang et al., 2023)。
在提示中，我们包含了策略描述，以增强对每种策略的理解，并随机选择了 2 次示例，因为在遵守开源模型所需的输出格式方面存在挑战。为了便于比较，我们还提供了闭源模型的 2 次示例。有关模型的更多详细信息请参见附录 C.3，有关提示的更多详细信息请参见附录 C.4。
4.2 RQ1：偏好是否会影响提供情感支持？
LLM 的熟练程度。图 3a 说明了每个 LLM 的熟练程度 Q（红线）。毫不奇怪，GPT-4 在熟练程度 Q 中得分最高，表明它具有与策略保持一致的总体最高能力，而较小的模型往往获得较低的分数。然而，即使在大小相似的模型中，LLM 也表现出不同的性能，Solar 和 LLaMA2-7B 等较小的模型表现出相对较好的熟练程度。
性能因测试集而异。图 3a 还展示了 LLM 在每个测试集上的表现，不同的形状代表不同的测试集 Dt。大多数 LLM 在 D2 或 D3 上取得高分，而在 D1 上的得分大多较低。这表明 LLM 表现出

议题：共创讨论
大家提前想想，deepseek那些博士是如何成功的，我们该怎么做才能在汽车领域做到成功
议题2：
陈博说大家也可以思考下，如何在吉利发挥更大的价值，需要什么资源，想做什么以及如何做。
围绕2025如何做好全域AI，大家先好好讨论，目标是什么？怎么做？和其他单位如何协同
