worker-1 >> (ReferenceModelRayActor pid=15069) [rank0]:[E ProcessGroupNCCL.cpp:523] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1018, OpType=_ALLGATHER_BASE, NumelIn=58343424, NumelOut=233373696, Timeout(ms)=1800000) ran for 1800186 milliseconds before timing out.
worker-1 >> (ReferenceModelRayActor pid=15069) [rank0]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
worker-1 >> (ReferenceModelRayActor pid=15069) [rank0]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
worker-1 >> (ReferenceModelRayActor pid=15069) [rank0]:[E ProcessGroupNCCL.cpp:1182] [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1018, OpType=_ALLGATHER_BASE, NumelIn=58343424, NumelOut=233373696, Timeout(ms)=1800000) ran for 1800186 milliseconds before timing out.
worker-1 >> (ReferenceModelRayActor pid=15069) Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
worker-1 >> (ReferenceModelRayActor pid=15069) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fecbc529d87 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fbd1aa01f66 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fbd1aa054bd in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fbd1aa060b9 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #4: <unknown function> + 0xdbbf4 (0x7feccf4a0bf4 in /opt/conda/bin/../lib/libstdc++.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #5: <unknown function> + 0x94ac3 (0x7fecd1319ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #6: <unknown function> + 0x126a40 (0x7fecd13aba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,186 E 15069 16063] logging.cc:101: Unhandled exception: N3c1016DistBackendErrorE. what(): [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1018, OpType=_ALLGATHER_BASE, NumelIn=58343424, NumelOut=233373696, Timeout(ms)=1800000) ran for 1800186 milliseconds before timing out.
worker-1 >> (ReferenceModelRayActor pid=15069) Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
worker-1 >> (ReferenceModelRayActor pid=15069) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fecbc529d87 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fbd1aa01f66 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fbd1aa054bd in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fbd1aa060b9 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #4: <unknown function> + 0xdbbf4 (0x7feccf4a0bf4 in /opt/conda/bin/../lib/libstdc++.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #5: <unknown function> + 0x94ac3 (0x7fecd1319ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #6: <unknown function> + 0x126a40 (0x7fecd13aba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
worker-1 >> (ReferenceModelRayActor pid=15069) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fecbc529d87 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #1: <unknown function> + 0xdcc083 (0x7fbd1a75e083 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #2: <unknown function> + 0xdbbf4 (0x7feccf4a0bf4 in /opt/conda/bin/../lib/libstdc++.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #3: <unknown function> + 0x94ac3 (0x7fecd1319ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #4: <unknown function> + 0x126a40 (0x7fecd13aba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,196 E 15069 16063] logging.cc:108: Stack trace: 
worker-1 >> (ReferenceModelRayActor pid=15069)  /opt/conda/lib/python3.10/site-packages/ray/_raylet.so(+0xfe573a) [0x7fecd05cb73a] ray::operator<<()
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/lib/python3.10/site-packages/ray/_raylet.so(+0xfe81f8) [0x7fecd05ce1f8] ray::TerminateHandler()
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/bin/../lib/libstdc++.so.6(+0xb135a) [0x7feccf47635a] __cxxabiv1::__terminate()
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/bin/../lib/libstdc++.so.6(+0xb13c5) [0x7feccf4763c5]
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/bin/../lib/libstdc++.so.6(+0xb134f) [0x7feccf47634f]
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so(+0xdcc13e) [0x7fbd1a75e13e] c10d::ProcessGroupNCCL::ncclCommWatchdog()
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/bin/../lib/libstdc++.so.6(+0xdbbf4) [0x7feccf4a0bf4] execute_native_thread_routine
worker-1 >> (ReferenceModelRayActor pid=15069) /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7fecd1319ac3]
worker-1 >> (ReferenceModelRayActor pid=15069) /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126a40) [0x7fecd13aba40]
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) *** SIGABRT received at time=1724960299 on cpu 102 ***
worker-1 >> (ReferenceModelRayActor pid=15069) PC: @     0x7fecd131b9fc  (unknown)  pthread_kill
worker-1 >> (ReferenceModelRayActor pid=15069)     @     0x7fecd12c7520  (unknown)  (unknown)
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,197 E 15069 16063] logging.cc:365: *** SIGABRT received at time=1724960299 on cpu 102 ***
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,197 E 15069 16063] logging.cc:365: PC: @     0x7fecd131b9fc  (unknown)  pthread_kill
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,197 E 15069 16063] logging.cc:365:     @     0x7fecd12c7520  (unknown)  (unknown)
worker-1 >> (ReferenceModelRayActor pid=15069) Fatal Python error: Aborted
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) [rank0]:[E ProcessGroupNCCL.cpp:523] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1018, OpType=_ALLGATHER_BASE, NumelIn=58343424, NumelOut=233373696, Timeout(ms)=1800000) ran for 1800186 milliseconds before timing out.
worker-1 >> (ReferenceModelRayActor pid=15069) [rank0]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
worker-1 >> (ReferenceModelRayActor pid=15069) [rank0]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
worker-1 >> (ReferenceModelRayActor pid=15069) [rank0]:[E ProcessGroupNCCL.cpp:1182] [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1018, OpType=_ALLGATHER_BASE, NumelIn=58343424, NumelOut=233373696, Timeout(ms)=1800000) ran for 1800186 milliseconds before timing out.
worker-1 >> (ReferenceModelRayActor pid=15069) Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
worker-1 >> (ReferenceModelRayActor pid=15069) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fecbc529d87 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fbd1aa01f66 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fbd1aa054bd in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fbd1aa060b9 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #4: <unknown function> + 0xdbbf4 (0x7feccf4a0bf4 in /opt/conda/bin/../lib/libstdc++.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #5: <unknown function> + 0x94ac3 (0x7fecd1319ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #6: <unknown function> + 0x126a40 (0x7fecd13aba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,186 E 15069 16063] logging.cc:101: Unhandled exception: N3c1016DistBackendErrorE. what(): [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1018, OpType=_ALLGATHER_BASE, NumelIn=58343424, NumelOut=233373696, Timeout(ms)=1800000) ran for 1800186 milliseconds before timing out.
worker-1 >> (ReferenceModelRayActor pid=15069) Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
worker-1 >> (ReferenceModelRayActor pid=15069) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fecbc529d87 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fbd1aa01f66 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fbd1aa054bd in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fbd1aa060b9 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #4: <unknown function> + 0xdbbf4 (0x7feccf4a0bf4 in /opt/conda/bin/../lib/libstdc++.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #5: <unknown function> + 0x94ac3 (0x7fecd1319ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #6: <unknown function> + 0x126a40 (0x7fecd13aba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
worker-1 >> (ReferenceModelRayActor pid=15069) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fecbc529d87 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #1: <unknown function> + 0xdcc083 (0x7fbd1a75e083 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #2: <unknown function> + 0xdbbf4 (0x7feccf4a0bf4 in /opt/conda/bin/../lib/libstdc++.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #3: <unknown function> + 0x94ac3 (0x7fecd1319ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) frame #4: <unknown function> + 0x126a40 (0x7fecd13aba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,196 E 15069 16063] logging.cc:108: Stack trace: 
worker-1 >> (ReferenceModelRayActor pid=15069)  /opt/conda/lib/python3.10/site-packages/ray/_raylet.so(+0xfe573a) [0x7fecd05cb73a] ray::operator<<()
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/lib/python3.10/site-packages/ray/_raylet.so(+0xfe81f8) [0x7fecd05ce1f8] ray::TerminateHandler()
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/bin/../lib/libstdc++.so.6(+0xb135a) [0x7feccf47635a] __cxxabiv1::__terminate()
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/bin/../lib/libstdc++.so.6(+0xb13c5) [0x7feccf4763c5]
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/bin/../lib/libstdc++.so.6(+0xb134f) [0x7feccf47634f]
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so(+0xdcc13e) [0x7fbd1a75e13e] c10d::ProcessGroupNCCL::ncclCommWatchdog()
worker-1 >> (ReferenceModelRayActor pid=15069) /opt/conda/bin/../lib/libstdc++.so.6(+0xdbbf4) [0x7feccf4a0bf4] execute_native_thread_routine
worker-1 >> (ReferenceModelRayActor pid=15069) /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7fecd1319ac3]
worker-1 >> (ReferenceModelRayActor pid=15069) /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126a40) [0x7fecd13aba40]
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) *** SIGABRT received at time=1724960299 on cpu 102 ***
worker-1 >> (ReferenceModelRayActor pid=15069) PC: @     0x7fecd131b9fc  (unknown)  pthread_kill
worker-1 >> (ReferenceModelRayActor pid=15069)     @     0x7fecd12c7520  (unknown)  (unknown)
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,197 E 15069 16063] logging.cc:365: *** SIGABRT received at time=1724960299 on cpu 102 ***
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,197 E 15069 16063] logging.cc:365: PC: @     0x7fecd131b9fc  (unknown)  pthread_kill
worker-1 >> (ReferenceModelRayActor pid=15069) [2024-08-29 19:38:19,197 E 15069 16063] logging.cc:365:     @     0x7fecd12c7520  (unknown)  (unknown)
worker-1 >> (ReferenceModelRayActor pid=15069) Fatal Python error: Aborted
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) 
worker-1 >> (ReferenceModelRayActor pid=15069) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, zstandard.backend_c, uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, sentencepiece._sentencepiece, _cffi_backend, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pyarrow.lib, pyarrow._hdfsio, pandas._libs.ops, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, scipy._lib._ccallback_c, numpy.linalg.lapack_lite, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._flinalg, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._ansari_swilk_statistics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper, sklearn.__check_build._check_build, sklearn.utils.murmurhash, sklearn.utils._isfinite, sklearn.utils._openmp_helpers, sklearn.utils._logistic_sigmoid, sklearn.utils.sparsefuncs_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.utils._typedefs, sklearn.utils._readonly_array_wrapper, sklearn.metrics._dist_metrics, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_fast, PIL._imaging (total: 214)
worker-1 >> (RewardModelRayActor pid=15673) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, zstandard.backend_c, uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, sentencepiece._sentencepiece, _cffi_backend, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pyarrow.lib, pyarrow._hdfsio, pandas._libs.ops, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, scipy._lib._ccallback_c, numpy.linalg.lapack_lite, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._flinalg, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._ansari_swilk_statistics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper, sklearn.__check_build._check_build, sklearn.utils.murmurhash, sklearn.utils._isfinite, sklearn.utils._openmp_helpers, sklearn.utils._logistic_sigmoid, sklearn.utils.sparsefuncs_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.utils._typedefs, sklearn.utils._readonly_array_wrapper, sklearn.metrics._dist_metrics, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_fast (total: 213)
worker-1 >> (RewardModelRayActor pid=15673) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, zstandard.backend_c, uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, sentencepiece._sentencepiece, _cffi_backend, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pyarrow.lib, pyarrow._hdfsio, pandas._libs.ops, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, scipy._lib._ccallback_c, numpy.linalg.lapack_lite, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._flinalg, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._ansari_swilk_statistics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper, sklearn.__check_build._check_build, sklearn.utils.murmurhash, sklearn.utils._isfinite, sklearn.utils._openmp_helpers, sklearn.utils._logistic_sigmoid, sklearn.utils.sparsefuncs_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.utils._typedefs, sklearn.utils._readonly_array_wrapper, sklearn.metrics._dist_metrics, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_fast (total: 213)
worker-1 >> (raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe583c1823b2811799ea2105102000000 Worker ID: d270042d88188df6066494a0e2581e606c9555ca0bd19f858261fbe6 Node ID: ef9eb6d612733b17fc2b950ffdd22edfc724ab47158420df0bb13d48 Worker IP address: 10.132.198.111 Worker port: 10118 Worker PID: 15069 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
worker-1 >> (RayWorkerWrapper pid=16693) init_process_group: master_address=10.132.198.111, master_port=43517,  rank=2, world_size=3, group_name=openrlhf [repeated 3x across cluster]
worker-1 >> Traceback (most recent call last):
worker-1 >>   File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
worker-1 >>     return _run_code(code, main_globals, None,
worker-1 >>   File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
worker-1 >>     exec(code, run_globals)
worker-1 >>   File "/tmp/ray/session_2024-08-29_19-01-25_047082_4751/runtime_resources/working_dir_files/_ray_pkg_1d035759fbe8accc/openrlhf/cli/train_ppo_ray.py", line 331, in <module>
worker-1 >>     train(args)
worker-1 >>   File "/tmp/ray/session_2024-08-29_19-01-25_047082_4751/runtime_resources/working_dir_files/_ray_pkg_1d035759fbe8accc/openrlhf/cli/train_ppo_ray.py", line 158, in train
worker-1 >>     ray.get(refs)
worker-1 >>   File "/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
worker-1 >>     return fn(*args, **kwargs)
worker-1 >>   File "/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
worker-1 >>     return func(*args, **kwargs)
worker-1 >>   File "/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py", line 2623, in get
worker-1 >>     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
worker-1 >>   File "/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py", line 861, in get_objects
worker-1 >>     raise value.as_instanceof_cause()
worker-1 >> ray.exceptions.RayTaskError(ActorDiedError): ray::ActorModelRayActor.fit() (pid=14792, ip=10.132.198.111, actor_id=6b08727a917b71974d9a92da02000000, repr=<openrlhf.trainer.ray.ppo_actor.ActorModelRayActor object at 0x7f2a058b9cf0>)
worker-1 >>   File "/tmp/ray/session_2024-08-29_19-01-25_047082_4751/runtime_resources/working_dir_files/_ray_pkg_1d035759fbe8accc/openrlhf/trainer/ray/ppo_actor.py", line 378, in fit
worker-1 >>     trainer.fit(
worker-1 >>   File "/tmp/ray/session_2024-08-29_19-01-25_047082_4751/runtime_resources/working_dir_files/_ray_pkg_1d035759fbe8accc/openrlhf/trainer/ppo_trainer.py", line 187, in fit
worker-1 >>     experience = self.experience_maker.make_experience(rand_prompts, **self.generate_kwargs)
worker-1 >>   File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
worker-1 >>     return func(*args, **kwargs)
worker-1 >>   File "/tmp/ray/session_2024-08-29_19-01-25_047082_4751/runtime_resources/working_dir_files/_ray_pkg_1d035759fbe8accc/openrlhf/trainer/ppo_utils/experience_maker.py", line 290, in make_experience
worker-1 >>     ref_values = ray.get([base_action_log_probs_ref, value_ref] + r_refs)
worker-1 >> ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.
worker-1 >> 	class_name: ReferenceModelRayActor
worker-1 >> 	actor_id: e583c1823b2811799ea2105102000000
worker-1 >> 	pid: 15069
worker-1 >> 	namespace: 6d37857c-1c09-4832-8bef-3f36ef3b98e6
worker-1 >> 	ip: 10.132.198.111
worker-1 >> The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
worker-1 >> (raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe4c5ca854b9769f20c81672702000000 Worker ID: 7ba2f5752cc458843a1b1c9b7950485b96b2030ee08be589a6a8e8ae Node ID: ef9eb6d612733b17fc2b950ffdd22edfc724ab47158420df0bb13d48 Worker IP address: 10.132.198.111 Worker port: 10123 Worker PID: 15673 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
worker-1 >> (RewardModelRayActor pid=15673) [rank0]:[E ProcessGroupNCCL.cpp:523] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1018, OpType=_ALLGATHER_BASE, NumelIn=58343424, NumelOut=233373696, Timeout(ms)=1800000) ran for 1800720 milliseconds before timing out. [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [rank0]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [rank0]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down. [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [rank0]:[E ProcessGroupNCCL.cpp:1182] [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1018, OpType=_ALLGATHER_BASE, NumelIn=58343424, NumelOut=233373696, Timeout(ms)=1800000) ran for 1800720 milliseconds before timing out. [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first): [repeated 4x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc5942f4d87 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so) [repeated 6x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f95fea01f66 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so) [repeated 4x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f95fea060b9 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so) [repeated 8x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) frame #4: <unknown function> + 0x126a40 (0x7fc5b2435a40 in /usr/lib/x86_64-linux-gnu/libc.so.6) [repeated 20x across cluster]
worker-1 >> (RewardModelRayActor pid=15673)  [repeated 24x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [2024-08-29 19:38:19,717 E 15673 16709] logging.cc:101: Unhandled exception: N3c1016DistBackendErrorE. what(): [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1018, OpType=_ALLGATHER_BASE, NumelIn=58343424, NumelOut=233373696, Timeout(ms)=1800000) ran for 1800720 milliseconds before timing out. [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first): [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [2024-08-29 19:38:19,728 E 15673 16709] logging.cc:108: Stack trace:  [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673)  /opt/conda/lib/python3.10/site-packages/ray/_raylet.so(+0xfe573a) [0x7fc5b165573a] ray::operator<<() [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) /opt/conda/lib/python3.10/site-packages/ray/_raylet.so(+0xfe81f8) [0x7fc5b16581f8] ray::TerminateHandler() [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) /opt/conda/bin/../lib/libstdc++.so.6(+0xdbbf4) [0x7fc5b052abf4] execute_native_thread_routine [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) *** SIGABRT received at time=1724960299 on cpu 28 *** [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) PC: @     0x7fc5b23a59fc  (unknown)  pthread_kill [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673)     @     0x7fc5b2351520  (unknown)  (unknown) [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [2024-08-29 19:38:19,729 E 15673 16709] logging.cc:365: *** SIGABRT received at time=1724960299 on cpu 28 *** [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [2024-08-29 19:38:19,729 E 15673 16709] logging.cc:365: PC: @     0x7fc5b23a59fc  (unknown)  pthread_kill [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [2024-08-29 19:38:19,729 E 15673 16709] logging.cc:365:     @     0x7fc5b2351520  (unknown)  (unknown) [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) Fatal Python error: Aborted [repeated 2x across cluster]
worker-1 >> 2024-08-29 19:38:28,713	ERR cli.py:68 -- ---------------------------------------
worker-1 >> 2024-08-29 19:38:28,713	ERR cli.py:69 -- Job 'raysubmit_9kpcckfamiMGRswP' failed
worker-1 >> 2024-08-29 19:38:28,713	ERR cli.py:70 -- ---------------------------------------
worker-1 >> 2024-08-29 19:38:28,713	INFO cli.py:83 -- Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):
worker-1 >> (RewardModelRayActor pid=15673)  /opt/conda/lib/python3.10/site-packages/ray/_raylet.so(+0xfe573a) [0x7fc5b165573a] ray::operator<<() [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) /opt/conda/lib/python3.10/site-packages/ray/_raylet.so(+0xfe81f8) [0x7fc5b16581f8] ray::TerminateHandler() [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) /opt/conda/bin/../lib/libstdc++.so.6(+0xdbbf4) [0x7fc5b052abf4] execute_native_thread_routine [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) *** SIGABRT received at time=1724960299 on cpu 28 *** [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) PC: @     0x7fc5b23a59fc  (unknown)  pthread_kill [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673)     @     0x7fc5b2351520  (unknown)  (unknown) [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [2024-08-29 19:38:19,729 E 15673 16709] logging.cc:365: *** SIGABRT received at time=1724960299 on cpu 28 *** [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [2024-08-29 19:38:19,729 E 15673 16709] logging.cc:365: PC: @     0x7fc5b23a59fc  (unknown)  pthread_kill [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) [2024-08-29 19:38:19,729 E 15673 16709] logging.cc:365:     @     0x7fc5b2351520  (unknown)  (unknown) [repeated 2x across cluster]
worker-1 >> (RewardModelRayActor pid=15673) Fatal Python error: Aborted [repeated 2x across cluster]
